{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yarowsky Algorithm Pyspark Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark, random\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(appName=\"YourTest\", master=\"local[2]\", conf=SparkConf().set('spark.ui.port', random.randrange(4000,5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='bank_final.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sc.textFile(dataset).map(lambda x: (x[:-3],x[-1:]))\\\n",
    ".map(lambda x: (x[0],1 if x[1]==\"+\" else -1 if x[1]=='-' else 0))\n",
    "sents_indexed = sents.zipWithIndex()\n",
    "sents_train = sents_indexed.filter(lambda x: (x[0])[1] == 0 or x[1]%5!=0).map(lambda x: x[0])\n",
    "sents_test = sents_indexed.filter(lambda x: (x[0])[1] != 0 and x[1]%5==0).map(lambda x: x[0])\n",
    "model_path='model/group_x_model'\n",
    "result_path='result/group_x_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(s):\n",
    "    return re.findall(r\"[a-z]+(?:'[a-z]+)?\",s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamminess(f: list, w: dict):\n",
    "    \"\"\"compute spamminess of feature list f using weights w\"\"\" \n",
    "    score = 0\n",
    "    for feature in f:\n",
    "        score += w.get(feature,0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNGram(sents,m,N):\n",
    "    def generate_ngrams_lc(sent):\n",
    "        ngrams_list = []\n",
    "        words_list = simple_tokenize(sent[0])\n",
    "        for num in range(0, len(words_list)-N+1):\n",
    "            ngram = ' '.join(words_list[num:num + N])\n",
    "            ngrams_list.append(ngram)\n",
    "        return (ngrams_list,sent[1])\n",
    "    \n",
    "    def helper(imp,i2):\n",
    "        out = []\n",
    "        lst = imp\n",
    "        for string in lst:\n",
    "            ele = ((string, i2), 1)\n",
    "            out.append(ele)\n",
    "        return out\n",
    "\n",
    "    def helper2 (inp1, inp2):\n",
    "        return (inp1[0]+inp2[0],inp1[1]+inp2[1])\n",
    "\n",
    "    def helper3 (inp):\n",
    "        p1=(inp[1])[0]+0.0\n",
    "        n1=(inp[1])[1]+0.0\n",
    "        p2 = Positives - p1\n",
    "        n2 = Negatives - n1\n",
    "        w1 = (p1+n1)/(Positives+Negatives)\n",
    "        w2 = 1 - w1\n",
    "        prob1 = p1/(p1+n1)\n",
    "        prob2 = 0\n",
    "        if p2!=0:\n",
    "            prob2 = p2/(p2+n2)\n",
    "        neg_entropy_1 = prob1*np.log10(prob1)/np.log10(2.0) + (1-prob1)*np.log10(1-prob1)/np.log10(2.0)\n",
    "        neg_entropy_2 = prob1*np.log10(prob2)/np.log10(2.0) + (1-prob2)*np.log10(1-prob2)/np.log10(2.0)\n",
    "        IG = w1 * neg_entropy_1 + w2 * neg_entropy_2\n",
    "        if np.isnan(IG):\n",
    "            IG = float('-inf')\n",
    "        return (inp[0], IG)\n",
    "\n",
    "    Positives = sents.filter(lambda x: x[1]>0).count()\n",
    "    Negatives = sents.filter(lambda x: x[1]<0).count()\n",
    "    interm = sents.filter(lambda x: x[1]!=0).map(generate_ngrams_lc)\\\n",
    "    .flatMap(lambda x: helper(np.unique(x[0]),x[1])).reduceByKey(add)\\\n",
    "    .map(lambda x: ((x[0])[0], (x[1], 0)) if (x[0])[1] >0 else ((x[0])[0], (0, x[1])))\\\n",
    "    .reduceByKey(helper2).map(helper3).top(m, key=lambda x: x[1])\n",
    "    output= sc.parallelize(interm).map(lambda x: x[0]).zipWithIndex().collectAsMap()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(f_classified, w):\n",
    "    corrects = f_classified.map(lambda x: ('',1 if spamminess(x[0],w)*x[1]>0 else 0))\\\n",
    "    .reduceByKey(add).take(1)\n",
    "    return (corrects[0])[1]/f_classified.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainer(f_classified, n_itr, alpha, delta,N):\n",
    "    w={}\n",
    "    for itr in range(0,n_itr):\n",
    "        d_w = f_classified.map(lambda x: (x[0], 1 if x[1]>0 else 0))\\\n",
    "            .map(lambda x: (x[0],x[1],1/(1+exp(-spamminess(x[0],w)))))\\\n",
    "            .map(lambda x: (x[0],(x[1]-x[2])*alpha))\\\n",
    "            .flatMap(lambda x: map(lambda y: (y,x[1]), x[0]))\\\n",
    "            .reduceByKey(add)\n",
    "        w = d_w.map(lambda x: (x[0],x[1]+w.get(x[0],0))).collectAsMap()\n",
    "        s_d_w =  d_w.map(lambda x: (\"\",abs(x[1]))).reduceByKey(add).take(1)\n",
    "        if (s_d_w[0])[1] < delta:\n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run (sents0, sents_test, model_path, result_path, save, N, m, niter, threshold, alpha, delta):\n",
    "    #initialize\n",
    "    n_unclassified = 0\n",
    "    n_unclassified_new = sents0.filter(lambda x: x[1]== 0).count()\n",
    "    itr = 0\n",
    "    sents = sents0\n",
    "    f_map = {}\n",
    "    w = {}\n",
    "    def generate_ngrams_lc(sent): #the goal is to general ngrams of the form ('ABC',0)\n",
    "        ngrams_list = []\n",
    "        words_list = simple_tokenize(sent[0])\n",
    "        for num in range(0, len(words_list)-N+1):\n",
    "            ngram = ' '.join(words_list[num:num + N])\n",
    "            ngrams_list.append(ngram)\n",
    "        return (ngrams_list,sent[1])\n",
    "    f_map = extractNGram(sents,m,N)\n",
    "    while(n_unclassified_new>0 and n_unclassified_new != n_unclassified):\n",
    "\n",
    "        def tonums(interm): #the goal is to convert ngrams to numbers\n",
    "            numlst = []\n",
    "            strlst = interm[0]\n",
    "            for ele in strlst:\n",
    "                val = f_map.get(ele)\n",
    "                numlst.append(val)\n",
    "            numlst = [x for x in numlst if x is not None]\n",
    "            return (numlst, interm[1])\n",
    "        f_classified = sents.filter(lambda x: x[1]!=0).map(generate_ngrams_lc).map(tonums)\n",
    "        w = trainer(f_classified, niter, alpha, delta,N) \n",
    "        train_acc =  accuracy(f_classified, w)\n",
    "\n",
    "        def generate_string_ngrams(sent): #generate ngrams with original text in the front\n",
    "            ngrams_list = []\n",
    "            words_list = simple_tokenize(sent[0])\n",
    "            for num in range(0, len(words_list)-N+1):\n",
    "                ngram = ' '.join(words_list[num:num + N])\n",
    "                ngrams_list.append(ngram)\n",
    "            return (sent[0],ngrams_list,sent[1])\n",
    "\n",
    "        def stringandnums(interm): #generate converted ngrams with text in the front\n",
    "            numlst = []\n",
    "            strlst = interm[1]\n",
    "            for ele in strlst:\n",
    "                val = f_map.get(ele)\n",
    "                numlst.append(val)\n",
    "            numlst = [x for x in numlst if x is not None]\n",
    "            return (interm[0], numlst, interm[2])\n",
    "        \n",
    "        f_input = sents.map(generate_string_ngrams).map(stringandnums)\n",
    "        \n",
    "        def findscore (inp): #calculate the new label \n",
    "            label = inp[2]\n",
    "            if label==0:\n",
    "                score = spamminess(inp[1],w)\n",
    "                if score > threshold:\n",
    "                    label = 1\n",
    "                elif score < -threshold:\n",
    "                    label = -1\n",
    "            return (inp[0], label)\n",
    "        sents_new = f_input.map(findscore)\n",
    "        \n",
    "        #update\n",
    "        n_unclassified = sents.filter(lambda x: x[1] == 0).count()\n",
    "        n_unclassified_new = sents_new.filter(lambda x: x[1]== 0).count()\n",
    "        \n",
    "        itr = itr+1\n",
    "        sents = sents_new\n",
    "        \n",
    "    f_test = sents_test.map(generate_ngrams_lc).map(tonums)\n",
    "    #calculate test accuracy\n",
    "    test_acc =  accuracy(f_test, w)\n",
    "    #save model and file \n",
    "    if save:\n",
    "        sents.map(lambda x: (x[0]+'+' if x[1]==1 else x[0]+'-' if x[1]==-1 else 0))\\\n",
    "        .saveAsTextFile(result_path)\n",
    "        model = f_map.map(lambda x: (x[0], w.get(x[1],0.0))).saveAsTextFile(model_path)\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#find the optimal parameters\n",
    "def parameter_search(\n",
    "    sents_train, sents_test, model_path, result_path,n_start, n_end, n_step, \n",
    "    m_start, m_end, m_step, niter_start, niter_end, niter_step, \n",
    "    threshold_start, threshold_end, threshold_step, \n",
    "    alpha_start, alpha_end, alpha_step,  \n",
    "    delta_start, delta_end, delta_step):\n",
    "    N_best = n_start\n",
    "    m_best = m_start\n",
    "    niter_best = niter_start\n",
    "    threshold_best = threshold_start\n",
    "    alpha_best = alpha_start\n",
    "    delta_best = delta_start\n",
    "    acc_best = 0.0\n",
    "\n",
    "    for N, m, niter, threshold, alpha, delta in itertools.product(\n",
    "        np.arange(n_start,n_end,n_step), np.arange(m_start,m_end,m_step),np.arange(niter_start,niter_end,niter_step),\n",
    "    np.arange(threshold_start,threshold_end,threshold_step),np.arange(alpha_start,alpha_end,alpha_step),\n",
    "        np.arange(delta_start,delta_end,delta_step)):\n",
    "        acc = run(sents_train, sents_test, model_path, result_path, 0 ,N, m, niter, threshold, alpha, delta)\n",
    "        if acc>acc_best:\n",
    "            N_best = N\n",
    "            m_best = m\n",
    "            niter_best = niter\n",
    "            threshold_best = threshold\n",
    "            alpha_best = alpha\n",
    "            delta_best = delta\n",
    "            acc_best = acc\n",
    "            \n",
    "    return (N_best,m_best,niter_best,threshold_best,alpha_best,delta_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_search(sents_train, sents_test, model_path, result_path, 1,6,1,100,1000,100,1,101,10,0.5,0.85,0.05,0.01,0.2,0.01,0.01,0.2,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
